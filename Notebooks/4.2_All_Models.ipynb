{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-01T08:55:30.254512Z",
     "start_time": "2024-05-01T08:55:30.248928Z"
    }
   },
   "source": [
    "import importlib\n",
    "import random\n",
    "from typing import Dict, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from shared.definitions import TuningResult\n",
    "from shared.ml_config_core import ModelTrainingResult\n",
    "from shared import stats_utils\n",
    "from shared.ml_config_runner import build_production_model_for_tuning_result\n",
    "from shared import graph\n",
    "from shared import utils\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from Draft import feature_builder_v2\n",
    "import importlib\n",
    "import xgboost as xgb\n",
    "from dataclasses import asdict\n",
    "\n",
    "VERBOSE = False\n",
    "utils.pandas_config(pd)\n",
    "utils.plt_config(plt)\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"pastel\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "# Use this to select the model to train, gennerally there are certain advantages and disadvantages between using different tuning targets:\n",
    "# e.g. F1 provides better classification accuracy\n",
    "# PR-AUC however is only slightly worse but has much smoother probabilities which is more\n",
    "# useful when when selecting thresholds for grades\n",
    "INCLUDE_MODELS = [\n",
    "    # \"Baseline_Only_CreditRatings\",\n",
    "    # \"LGBM_AUC_Base_Features\",\n",
    "    # \"LGBM_Weighted_LogLoss\",\n",
    "    # \"LGBM_AUC\",\n",
    "    \"LGBM_Dart_AUC\",\n",
    "    # \"LGBM_AUC_All_Features\",\n",
    "]\n",
    "importlib.reload(feature_builder_v2)\n",
    "# features_matrix = feature_builder_v2.load_datasets_and_prepare_features(drop_meta_data=True, ds_type=feature_builder_v2.DatasetType.FULL)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Draft.feature_builder_v2' from 'V:\\\\projects\\\\ppuodz-ML.4.1\\\\Draft\\\\feature_builder_v2.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "results_all_models: Dict[str, Tuple[TuningResult, ModelTrainingResult]] = {}\n",
    "\n",
    "for model_key in INCLUDE_MODELS:\n",
    "\n",
    "    # for s in [1]:\n",
    "    for s in [1, 2, 5, 7, 42, 64, 420, 273, 999, 111, 72, 27, 33, 42]:\n",
    "        m_key = f\"{model_key}_{s}\"\n",
    "        tuning_result = TuningResult.load_serialized_tuning_result(model_key)\n",
    "\n",
    "        cv_results = build_production_model_for_tuning_result(\n",
    "            tuning_result=tuning_result,\n",
    "            load_df=feature_builder_v2.load_datasets_and_prepare_features,\n",
    "        )\n",
    "        results_all_models[m_key] = (tuning_result, cv_results)\n",
    "\n",
    "        model_size = ModelTrainingResult.serialize_model(cv_results, m_key)\n",
    "\n",
    "        results_all_models[m_key][1].meta_data.total_size = model_size"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-01T08:55:30.407609Z"
    }
   },
   "id": "458256ff1c211de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n",
      "C:\\Users\\Paulius\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\ppuodz-ml-4-1-dqELbViF-py3.12\\Lib\\site-packages\\woodwork\\type_sys\\utils.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for model_key in INCLUDE_MODELS:\n",
    "    tuning_result = TuningResult.load_serialized_tuning_result(model_key)\n",
    "    print(f\"{model_key}:\\n  {tuning_result.get_best_params()}\")"
   ],
   "id": "ed7c73927ad47229",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def aggregate_results_to_table(\n",
    "        results: Dict[str, Tuple[TuningResult, ModelTrainingResult]],\n",
    "        validation_results=False,\n",
    "):\n",
    "    \"\"\" \"\n",
    "    Aggregates results from all models into a single table\n",
    "    :param results:\n",
    "    :param validation_results:\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for model_key, model_data in results.items():\n",
    "        model_results = model_data[1]\n",
    "\n",
    "        results = (\n",
    "            model_results.cv_metrics\n",
    "            if validation_results\n",
    "            else model_results.test_data.metrics_2\n",
    "        )\n",
    "\n",
    "        metrics = {f\"{k}\": round(v, 3) for k, v in results.items()}\n",
    "        metrics = {\n",
    "            \"Model\": model_key,\n",
    "            **metrics,\n",
    "            **asdict(model_results.meta_data),\n",
    "        }  # Add model key to the metrics dictionary\n",
    "        rows.append(metrics)\n",
    "    results_df = pd.DataFrame(rows).sort_values(by=\"auc\", ascending=False)\n",
    "    results_df.to_csv(\n",
    "        f\"data/results_all_models_{'validation' if validation_results else 'test'}.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "    return results_df"
   ],
   "id": "940c5ae576e00e58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Model Results Comparison (5-fold CV)",
   "id": "60bdea76097962e1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "aggregate_results_to_table(results_all_models, validation_results=True)",
   "id": "b703a7f6f7b33728",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Model Results Comparison (Test Set)",
   "id": "8763d137597b49f3"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "aggregate_results_to_table(results_all_models)",
   "id": "fad3fa19450f9339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d8e2f67d3e68a63f"
  },
  {
   "cell_type": "code",
   "source": [
    "cv_results_all_models = {k: v[1] for k, v in results_all_models.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "a9df11ffce67e5d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "if len(cv_results_all_models) < 2:\n",
    "    cv_results_all_models[\"_DUMMY\"] = cv_results_all_models[\n",
    "        list(cv_results_all_models.keys())[0]\n",
    "    ]\n",
    "importlib.reload(graph)\n",
    "first_model = list(cv_results_all_models.keys())[0]\n",
    "graph.roc_precision_recal_grid_plot(cv_results_all_models, show_observation_count=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "bab27e69257558ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "4e3e8cc45bb9c4e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "importlib.reload(graph)\n",
    "\n",
    "n = len(cv_results_all_models)\n",
    "columns = 2\n",
    "rows = (n + 1) // columns\n",
    "height = 8\n",
    "width = height * columns\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    rows, columns, figsize=(width, height * rows), constrained_layout=True\n",
    ")\n",
    "plt.suptitle(\"Confusion Matrices: Best Models based on f1\", fontsize=20)\n",
    "\n",
    "axes_flat = axes.flatten()\n",
    "for i, model_key in enumerate(cv_results_all_models.keys()):\n",
    "    graph.confusion_matrix_plot_v2(\n",
    "        cv_results_all_models[model_key].cm_data,\n",
    "        title=model_key,\n",
    "        annotations=graph.make_annotations(\n",
    "            cv_results_all_models[model_key].cv_metrics,\n",
    "            n_feats=len(cv_results_all_models[model_key].cm_data.x_test.columns),\n",
    "        ),\n",
    "        ax=axes_flat[i],\n",
    "    )\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(i + 1, len(axes_flat)):\n",
    "    axes_flat[j].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "46b96cd28dba0857",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "models_data = [\n",
    "    # {'name': 'Model A', 'y_test': y_test_A, 'prob_pos': prob_pos_A},\n",
    "    # {'name': 'Model B', 'y_test': y_test, 'prob_pos': prob_pos}\n",
    "]\n",
    "\n",
    "for k, model_config in results_all_models.items():\n",
    "    target_model_config = results_all_models[k]\n",
    "    features_matrix = target_model_config[0].model_pipeline_config.load_data(\n",
    "        loader_function=feature_builder_v2.load_datasets_and_prepare_features\n",
    "    )\n",
    "    #\n",
    "    features_all, labels_all = pipeline._get_features_labels(features_matrix)\n",
    "    X_train, X_test, y_train, y_test = pipeline.get_deterministic_train_test_split(\n",
    "        features_all, labels_all\n",
    "    )\n",
    "\n",
    "    X_train = X_train.drop(columns=[\"TARGET\"])\n",
    "    X_test = X_test.drop(columns=[\"TARGET\"])\n",
    "\n",
    "    model = target_model_config[1].test_data.test_model.named_steps[\"model\"]\n",
    "\n",
    "    prob_pos = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    models_data.append(dict(name=k, y_test=y_test, prob_pos=prob_pos))"
   ],
   "id": "d25bf0bc716917ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "ba5edfbea8878660",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d5aa77bffd1fad57",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "d3a129e1e3342943",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import gridspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss, log_loss\n",
    "\n",
    "\n",
    "def plot_calibration_plot(ax0, ax1, y_test, prob_pos, n_bins=10):\n",
    "    true_prob, pred_prob = calibration_curve(y_test, prob_pos, n_bins=n_bins)\n",
    "    calibration_data = pd.DataFrame(\n",
    "        {\"Predicted Probability\": pred_prob, \"True Probability\": true_prob}\n",
    "    )\n",
    "\n",
    "    # Calibration curve\n",
    "    sns.lineplot(\n",
    "        data=calibration_data, x=\"Predicted Probability\", y=\"True Probability\", ax=ax0\n",
    "    )\n",
    "    ax0.plot([0, 1], [0, 1], \"k--\")\n",
    "    ax0.set_title(\"Calibration Curve\")\n",
    "    ax0.set_xlabel(\"Mean Predicted Probability\")\n",
    "    ax0.set_ylabel(\"Fraction of Positives\")\n",
    "\n",
    "    # Annotations for ECE, Brier Score, and Log Loss\n",
    "    ece = np.mean(np.abs(pred_prob - true_prob))\n",
    "    brier_score = brier_score_loss(y_test, prob_pos)\n",
    "    logloss = log_loss(y_test, prob_pos)\n",
    "    ax0.annotate(\n",
    "        f\"ECE: {ece:.3f}\\nBrier Score: {brier_score:.3f}\\nLog Loss: {logloss:.3f}\",\n",
    "        xy=(0.05, 0.95),\n",
    "        xycoords=\"axes fraction\",\n",
    "        verticalalignment=\"top\",\n",
    "    )\n",
    "\n",
    "    # Histogram\n",
    "    sns.histplot(prob_pos, bins=10, ax=ax1)\n",
    "    ax1.set_xlabel(\"Predicted Probabilities\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "\n",
    "\n",
    "def plot_residuals(ax, y_test, prob_pos):\n",
    "    residuals = np.where(y_test == 1, -np.log(prob_pos), -np.log(1 - prob_pos))\n",
    "    ax.scatter(prob_pos, residuals, alpha=0.5)\n",
    "    ax.hlines(y=0, xmin=0, xmax=1, color=\"red\", linestyles=\"--\")\n",
    "    ax.set_xlabel(\"Predicted Probability\")\n",
    "    ax.set_ylabel(\"Residuals (Log Loss)\")\n",
    "    ax.set_title(\"Residual Plot for Binary Classification\")\n",
    "\n",
    "    # Annotations for MAE and Maximum Residual\n",
    "    mae = np.mean(np.abs(residuals))\n",
    "    max_residual = np.max(np.abs(residuals))\n",
    "    ax.annotate(\n",
    "        f\"MAE: {mae:.3f}\\nMax Residual: {max_residual:.3f}\",\n",
    "        xy=(0.05, 0.95),\n",
    "        xycoords=\"axes fraction\",\n",
    "        verticalalignment=\"top\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_model_comparisons(models_data):\n",
    "    for model in models_data:\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        gs = gridspec.GridSpec(2, 2, height_ratios=[4, 1], width_ratios=[1, 1])\n",
    "\n",
    "        # Calibration curve and histogram\n",
    "        ax0 = plt.subplot(gs[0, 0])\n",
    "        ax1 = plt.subplot(gs[1, 0], sharex=ax0)\n",
    "        plot_calibration_plot(ax0, ax1, model[\"y_test\"], model[\"prob_pos\"])\n",
    "\n",
    "        # Residuals plot\n",
    "        ax2 = plt.subplot(gs[:, 1])\n",
    "        plot_residuals(ax2, model[\"y_test\"], model[\"prob_pos\"])\n",
    "\n",
    "        plt.suptitle(f\"{model['name']}\", fontsize=18)\n",
    "        plt.tight_layout(\n",
    "            rect=[0, 0.03, 1, 0.95]\n",
    "        )  # Adjust layout to make room for title\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# models_data = [\n",
    "#     # {'name': 'Model A', 'y_test': y_test_A, 'prob_pos': prob_pos_A},\n",
    "#     {'name': 'Model B', 'y_test': y_test, 'prob_pos': prob_pos}\n",
    "# ]\n",
    "plot_model_comparisons(models_data)"
   ],
   "id": "58000174246b2674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c5eb20c0c93f9b43",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "77a1cb117749e9fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The **calibration curve**, also known as a reliability diagram, is a graphical representation used to evaluate the accuracy of predicted probabilities in classification models. It specifically checks how well the predicted probabilities of a model match the actual outcomes.\n",
    "\n",
    "The ideal calibration curve is a straight line at a 45-degree angle from the bottom left to the top right of the plot. This line, often called the \"line of perfect calibration,\" indicates that the model's predictions are perfectly calibrated. If your model predicts a class with 70% probability, then 70% of the cases that are predicted as such should indeed belong to that class.\n",
    "\n",
    "Common Patterns and Their Interpretations:\n",
    "Perfect Calibration:\n",
    "\n",
    "The points lie on the diagonal line from (0,0) to (1,1).\n",
    "Example: If a model predicts an event with 30% probability, then in the long run, that event occurs about 30% of the time when predicted at this probability.\n",
    "Underconfidence:\n",
    "\n",
    "The curve lies above the diagonal line.\n",
    "The model's probabilities are lower than the true likelihood of the event. For instance, if events the model predicts to happen 60% of the time actually happen 80% of the time, the model is underconfident.\n",
    "Overconfidence:\n",
    "\n",
    "The curve lies below the diagonal line.\n",
    "The model predicts higher probabilities than what is true. If a prediction of 80% only happens 60% of the time, the model is overconfident.\n",
    "\n",
    "*Expected Calibration Error (ECE)*: This measures the average difference between the predicted probabilities and the actual outcomes. Lower values indicate better calibration.\n",
    "\n",
    "*Brier Score*: Measures the mean squared difference between the predicted probability and the actual outcome. It is a good measure of the accuracy and calibration of the predictions.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d841116885f8a38c"
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "64ba1ac99114a3f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Residual** Plots show the difference between observed and predicted probabilities. Helps in checking the assumption of homoscedasticity. Ideally, residuals should be randomly dispersed around the central line, and patterns suggest model inadequacies.\n",
    "\n",
    "\n",
    "homoscedasticity\n",
    "\n",
    "\n",
    "Based on the chart we see these potential problems:\n",
    "Probability Estimates are Polarized:\n",
    "\n",
    "The cup-like pattern at the top suggests the model is very confident (probabilities close to 0 or 1) about certain instances but is incorrect, as these points have higher residuals.\n",
    "The bottom lines being straighter and closer to zero indicate that for a range of predicted probabilities, the residuals are consistently low, which means the model performs well in that range.\n",
    "\n",
    "Model Overconfidence:\n",
    "\n",
    "The residuals are larger for predictions near 0 or 1 because the log loss penalizes confident incorrect predictions more harshly than less confident ones.\n",
    "This overconfidence is often characteristic of models that are not well-calibrated and could benefit from probability calibration techniques.\n",
    "\n",
    "Class Imbalance:\n",
    "\n",
    "This pattern can sometimes emerge from class imbalance if the model is better at predicting the majority class and frequently mispredicts the minority class with high confidence.\n",
    "\n",
    "Non-linearity in Feature Space:\n",
    "\n",
    "The curving pattern could also be a sign that the model is not capturing some non-linear relationships between features and the outcome. This might suggest that feature engineering or a more sophisticated model could be helpful.\n",
    "\n",
    "*Mean Absolute Error (MAE)* of Residuals: This is the average of the absolute values of the residuals. It gives an idea of the average magnitude of the prediction errors.\n",
    "Annotation: Indicating MAE on the residual plot can help assess the typical error magnitude in a more intuitive way than just viewing the spread of residuals.\n",
    "\n",
    "*Maximum Residual*: The maximum value among the residuals can indicate the worst-case scenario for your predictions.\n",
    "Annotation: Marking the maximum residual can alert users to the worst errors the model could make.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af054848d27abcc1"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "88d401d14ed489cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "id": "70ef200a1dfabc11"
  },
  {
   "cell_type": "code",
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "709e321b0b0c8995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2503849e9b41286b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "shap.summary_plot(shap_values[1], X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "1c7f66dbe43441fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming shap_values is a list of SHAP values for a multi-class model, or a numpy array for a binary model\n",
    "if isinstance(shap_values, list):  # Multi-class scenario\n",
    "    # Calculate the mean absolute SHAP values for each feature across all classes\n",
    "    shap_abs_mean = np.abs(np.concatenate(shap_values, axis=0)).mean(axis=0)\n",
    "else:  # Binary classification or regression\n",
    "    shap_abs_mean = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Creating a DataFrame with feature names and their corresponding mean absolute SHAP values\n",
    "feature_importances_df = pd.DataFrame(\n",
    "    list(zip(X_test.columns, shap_abs_mean)), columns=[\"Feature\", \"Importance\"]\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "# display(feature_importances_df)\n",
    "\n",
    "low_imp = feature_importances_df[feature_importances_df[\"Importance\"] < 0.002]\n",
    "list(low_imp[\"Feature\"])"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "97d53757b5f86e20",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "high_imp = feature_importances_df[feature_importances_df[\"Importance\"] > 0.035]\n",
    "list(high_imp[\"Feature\"])\n",
    "# len(list(high_imp[\"Feature\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "7001c1fac2ec67a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "any_imp = feature_importances_df[feature_importances_df[\"Importance\"] > 0.01]\n",
    "list(any_imp[\"Feature\"])\n",
    "# len(list(any_imp[\"Feature\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "a0ad2fbd496e79f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate absolute differences between predicted probabilities and actual values\n",
    "predicted_probabilities = model.predict_proba(X_test)[\n",
    "                          :, 1\n",
    "                          ]  # Probabilities for the positive class\n",
    "\n",
    "absolute_differences = np.abs(predicted_probabilities - y_test)\n",
    "\n",
    "# Create a DataFrame for easier handling\n",
    "differences_df = pd.DataFrame(\n",
    "    {\"absolute_difference\": absolute_differences, \"index\": range(len(y_test))}\n",
    ")\n",
    "\n",
    "# Sort by 'absolute_difference' in descending order and select the top N\n",
    "N = 10\n",
    "top_differences_df = differences_df.nlargest(N, \"absolute_difference\")\n",
    "\n",
    "# If you want a random sample of N from these top differences\n",
    "random_sample_top_diff = top_differences_df.sample(\n",
    "    n=N, random_state=42\n",
    ")  # Use a fixed seed for reproducibility\n",
    "\n",
    "# Get the indices of the selected samples\n",
    "selected_indices = random_sample_top_diff[\"index\"].values\n",
    "\n",
    "# Use these indices to select instances from X_test or y_test if needed\n",
    "selected_samples_X_test = X_test.iloc[selected_indices]\n",
    "selected_samples_y_test = y_test.iloc[selected_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "704085f27cd6ef22",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# for i in selected_indices:\n",
    "# Create the SHAP Explanation object for the selected sample\n",
    "shap_explanation = shap.Explanation(\n",
    "    values=shap_values[1],\n",
    "    base_values=explainer.expected_value[1],\n",
    "    data=X_test.iloc[i].values,\n",
    "    feature_names=X_test.columns.tolist(),\n",
    ")\n",
    "\n",
    "# Calculate predicted probability for the selected sample\n",
    "predicted_probability = model.predict_proba(X_test.iloc[i: i + 1])[0, 1]\n",
    "\n",
    "# Retrieve the actual y value for the selected sample\n",
    "actual_y_value = y_test.iloc[i]\n",
    "\n",
    "# Generate the waterfall plot\n",
    "shap.plots.waterfall(shap_explanation, show=False)\n",
    "\n",
    "# Adding a title with the sample index, actual y value, and predicted probability\n",
    "plt.title(\n",
    "    f\"{i}, Actual Y Value: {actual_y_value}, Predicted Probability: {predicted_probability:.2f}\"\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "d4c59ca31e5b46df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "cm_target_model_key = \"LGBMDTuneAUC_Full_DropLowImp\"\n",
    "cm_target_model = cv_results_all_models[cm_target_model_key]\n",
    "\n",
    "target_cm_data = cm_target_model.test_data\n",
    "target_cm_data_sorted = target_cm_data.probabilities.sort_values(by=1).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "target_cm_data_sorted\n",
    "# target_cm_data_sorted[\"value\"] = target_cm_data.y_test"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "7e646ac5b12717ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "prediction_data = target_cm_data.probabilities.copy()\n",
    "prediction_data[\"value\"] = target_cm_data.y_test"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "6a35a85b95fbc919",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "prediction_data"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "845923752161969d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "target_cm_data.probabilities"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "864173d8e497e467",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the finance industry, loan grades (or credit scores) are a crucial part of risk management, helping lenders assess the creditworthiness of borrowers. These grades are typically determined based on various factors, including the borrower's credit history, income stability, debt-to-income ratio, and more. The grades reflect the estimated risk of default, and they directly influence the interest rate offered to the borrower. Commonly, loan grades are categorized from 'A' (lowest risk) to 'G' (highest risk), although the specific categories can vary by institution.\n",
    "\n",
    "A (Lowest Risk): Below 1% default rate. Borrowers with excellent credit histories and very low risk of default.\n",
    "B: 1% to 3% default rate. Borrowers with good credit histories and low risk of default.\n",
    "C: 3% to 7% default rate. Borrowers with average credit histories and moderate risk of default.\n",
    "D: 7% to 15% default rate. Borrowers with below-average credit histories and higher risk of default.\n",
    "E: 15% to 25% default rate. Borrowers with poor credit histories and very high risk of default.\n",
    "F and G (Highest Risk): Above 25% default rate. Borrowers with very poor credit histories and extremely high risk of default.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d87d2068f5ba7444"
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming source_df_grades_summary and target_cm_data are already defined\n",
    "# Sort probabilities for cumulative proportion mapping\n",
    "\n",
    "# Define the data as a dictionary\n",
    "data = {\n",
    "    \"index\": [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"],\n",
    "    \"Proportion\": [0.12, 0.14, 0.17, 0.14286, 0.14286, 0.14286, 0.14286],\n",
    "    \"Loan_Status_Ratio\": [1, 1, 1, 1, 1, 1, 1],\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "source_df_grades_summary = pd.DataFrame(data)\n",
    "\n",
    "# Set the 'index' column as the index of the DataFrame\n",
    "source_df_grades_summary = source_df_grades_summary.set_index(\"index\")\n",
    "\n",
    "source_df_grades_summary[\"Cumulative\"] = source_df_grades_summary[\"Proportion\"].cumsum()\n",
    "source_df_grades_summary[\"Cumulative\"] = source_df_grades_summary[\"Cumulative\"].clip(\n",
    "    upper=1\n",
    ")\n",
    "\n",
    "sections = []\n",
    "\n",
    "palette = sns.color_palette(\"YlOrRd\", n_colors=len(source_df_grades_summary))\n",
    "\n",
    "start_color = \"grey\"\n",
    "colors = (\n",
    "        [start_color] + palette + [\"red\"]\n",
    ")  # Ensure \"red\" is used for the merged EFG section\n",
    "\n",
    "i = 0\n",
    "previous_cumulative = 0\n",
    "\n",
    "# efg_weights = source_df_grades_summary.loc[\"E\":\"G\", \"Counts\"]\n",
    "# efg_risks = source_df_grades_summary.loc[\"E\":\"G\", \"Loan_Status_Ratio\"]\n",
    "# weighted_avg_risk_efg = np.average(efg_risks, weights=efg_weights)\n",
    "\n",
    "# Find thresholds in target_cm_data that match cumulative proportions\n",
    "for grade, row in source_df_grades_summary.iterrows():\n",
    "    if grade == source_df_grades_summary.index[0]:\n",
    "        start = 0.0\n",
    "    else:\n",
    "        start = target_cm_data_sorted[1].quantile(previous_cumulative)\n",
    "\n",
    "    end = target_cm_data_sorted[1].quantile(row[\"Cumulative\"])\n",
    "\n",
    "    #  merge grades E, F, G\n",
    "    # if grade == \"E\":\n",
    "    #     continue\n",
    "    # elif grade == \"F\":\n",
    "    #     continue\n",
    "    # elif grade == \"G\":\n",
    "    #     label = \"E-F-G Grade\"\n",
    "    #     color = \"red\"\n",
    "    # else:\n",
    "    if True:\n",
    "        label = f\"{grade} Grade\"\n",
    "        color = colors[i]\n",
    "\n",
    "    # if grade == \"G\":\n",
    "    #     description = f\"Expected D. R.: {weighted_avg_risk_efg:.1%}\"\n",
    "    # else:\n",
    "    #     description = f\"Expected D. R.: {row['Loan_Status_Ratio']:.1%}\"\n",
    "\n",
    "    sections.append(\n",
    "        {\n",
    "            \"label\": label,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"color\": color,\n",
    "            \"description\": \"\",\n",
    "        }\n",
    "    )\n",
    "    if grade != \"G\":\n",
    "        i += 1\n",
    "    previous_cumulative = row[\"Cumulative\"]\n",
    "sections[-1][\"end\"] = 1.35\n",
    "# sections"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "d92ad8b49fcabfba",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# def calculate_thresholds(prediction_data: pd.DataFrame) -> list:\n",
    "#     # Default rates dictionary included within the function\n",
    "#     default_rates = {'A': 0.01,\n",
    "#                      'B': 0.03,\n",
    "#                      'C': 0.07,\n",
    "#                      'D': 0.15,\n",
    "#                      'E-F-G': 1}\n",
    "#                      # 'E': 0.25,\n",
    "#                      # 'F': 0.35,\n",
    "#                      # 'G': 1.0}\n",
    "#\n",
    "#     palette = sns.color_palette(\"YlOrRd\", n_colors=len(default_rates))\n",
    "#\n",
    "#     start_color = \"grey\"\n",
    "#     colors = (\n",
    "#         [start_color] + palette + [\"red\"]\n",
    "#     )  # Ensure \"red\" is used for the merged EFG section\n",
    "#\n",
    "#     # Prepare the data\n",
    "#     prediction_data_sorted = prediction_data.sort_values(by=1, ascending=True)\n",
    "#     prediction_data_sorted['cumulative_default'] = prediction_data_sorted['value'].cumsum()\n",
    "#     prediction_data_sorted['cumulative_non_default'] = np.arange(1, len(prediction_data_sorted) + 1) - prediction_data_sorted['cumulative_default']\n",
    "#     prediction_data_sorted['default_rate'] = prediction_data_sorted['cumulative_default'] / (prediction_data_sorted['cumulative_non_default'] + prediction_data_sorted['cumulative_default'])\n",
    "#\n",
    "#     # Initialize the list to store sections\n",
    "#     sections = []\n",
    "#     start = 0.0  # Initial start value\n",
    "#\n",
    "#     i = 0\n",
    "#     for grade, rate in default_rates.items():\n",
    "#         # Identify the threshold for the current grade\n",
    "#         mask = prediction_data_sorted['default_rate'] <= rate\n",
    "#         if mask.any():\n",
    "#             max_prob = prediction_data_sorted.loc[mask, 1].max()\n",
    "#             end = max_prob\n",
    "#         else:\n",
    "#             end = 1.0\n",
    "#\n",
    "#\n",
    "#\n",
    "#         # Append the section\n",
    "#         sections.append({\n",
    "#             \"label\": f\"{grade} Grade\",\n",
    "#             \"start\": start,\n",
    "#             \"end\": end,\n",
    "#             \"color\": colors[i],\n",
    "#             \"description\": f\"Default rate up to {rate:.1%}\"\n",
    "#         })\n",
    "#\n",
    "#         i+=1\n",
    "#         start = end  # Update start for the next grade\n",
    "#\n",
    "#     # Ensure the last section end is set correctly\n",
    "#     if sections:\n",
    "#         sections[-1]['end'] = 1.0\n",
    "#\n",
    "#     return sections\n",
    "#\n",
    "# # Assuming prediction_data is defined\n",
    "# Example usage\n",
    "def calculate_thresholds(prediction_data: pd.DataFrame) -> list:\n",
    "    # Default rates dictionary is now internal to the function\n",
    "    default_rates = {\"A\": 0.01, \"B\": 0.03, \"C\": 0.07, \"D\": 0.15, \"E-F-G\": 1.0}\n",
    "    # default_rates = {'A': 0.01, 'B': 0.03, 'C': 0.07, 'D': 0.15, 'E': 0.25, 'F': 0.35, 'G': 1.0}\n",
    "\n",
    "    # Adjust the color palette as specified\n",
    "    palette = sns.color_palette(\"YlOrRd\", n_colors=len(default_rates) - 2)\n",
    "    start_color = \"grey\"\n",
    "    colors = [start_color] + palette + [\"red\"]\n",
    "\n",
    "    # Sort by predicted probability of default\n",
    "    prediction_data_sorted = prediction_data.sort_values(by=1, ascending=True)\n",
    "\n",
    "    sections = []\n",
    "    previous_threshold = 0.0\n",
    "    color_index = 0\n",
    "\n",
    "    for grade, max_default_rate in default_rates.items():\n",
    "        # Filter data for loans not yet assigned to a more secure grade\n",
    "        remaining_loans = prediction_data_sorted[\n",
    "            prediction_data_sorted[1] > previous_threshold\n",
    "            ]\n",
    "\n",
    "        # Calculate cumulative defaults and loans for remaining loans\n",
    "        remaining_loans[\"cumulative_defaults\"] = remaining_loans[\"value\"].cumsum()\n",
    "        total_loans = np.arange(1, len(remaining_loans) + 1)\n",
    "        remaining_loans[\"default_rate\"] = (\n",
    "                remaining_loans[\"cumulative_defaults\"] / total_loans\n",
    "        )\n",
    "\n",
    "        # Find the threshold where the default rate exceeds the grade's max default rate\n",
    "        valid_loans = remaining_loans[\n",
    "            remaining_loans[\"default_rate\"] <= max_default_rate\n",
    "            ]\n",
    "        if not valid_loans.empty:\n",
    "            threshold = valid_loans[1].max()\n",
    "        else:\n",
    "            # If no loans meet the criteria, the threshold is the previous threshold\n",
    "            threshold = previous_threshold\n",
    "\n",
    "        # Append section with calculated thresholds and color\n",
    "        sections.append(\n",
    "            {\n",
    "                \"label\": f\"{grade} Grade\",\n",
    "                \"start\": previous_threshold,\n",
    "                \"end\": threshold,\n",
    "                \"color\": colors[color_index],\n",
    "                \"description\": f\"Default rate up to {max_default_rate * 100:.0f}%\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Update for the next iteration\n",
    "        previous_threshold = threshold\n",
    "        color_index += 1\n",
    "\n",
    "    # Ensure the last section goes up to 1\n",
    "    if sections:\n",
    "        sections[-1][\"end\"] = 1.0\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "sections = calculate_thresholds(prediction_data)\n",
    "print(sections)\n",
    "importlib.reload(stats_utils)\n",
    "importlib.reload(graph)\n",
    "summary_desc = \"`The chart shows the  performance of the if only individual with stroke Prob. > T are selected. Additionally the overlay indicates the number of people whose predicted P is in an given range. The overlays can be used to selected the most at risk individual based on the probability predicted for them`\"\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "graph.plot_threshold_metrics_v2(\n",
    "    target_cm_data,\n",
    "    0,\n",
    "    1,\n",
    "    sections=sections,\n",
    "    model_name=cm_target_model_key,\n",
    "    class_pos=1,\n",
    "    include_vars=[\"f1\", \"precision\", \"recall\"],\n",
    "    show_threshold_n=True,\n",
    "    log_x=False,\n",
    ")\n",
    "display(Markdown(summary_desc))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "7cbc30128be6cf09",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "cab98b9cff451e48",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# x = target_model_config\n",
    "print(1)\n",
    "target_model_config = results_all_models[\"LGBM_Dart_AUC_NEW_273\"]"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "5b26f28fd3209bd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "target_model_config[1].test_model.predict_proba()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "c4ca6494e53dffc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "### EXPORT TEST SET\n",
    "\n",
    "test_df = feature_builder_v2.load_datasets_and_prepare_features(\n",
    "    **{\n",
    "        **target_model_config[0].model_pipeline_config.data_loader_params,\n",
    "        \"ds_source\": feature_builder_v2.TargetDataset.TEST,\n",
    "        \"ds_type\": feature_builder_v2.DatasetType.FULL,\n",
    "        \"drop_meta_data\": False,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "f1598f8def408fe3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "93968be81cc41cbc"
  },
  {
   "cell_type": "code",
   "source": [
    "test_df_no_id = test_df.drop(columns=[\"SkIdCurr\"])\n",
    "test_predictions = target_model_config[1].test_data.test_model.predict_proba(\n",
    "    test_df_no_id\n",
    ")\n",
    "test_ids = test_df[\"SkIdCurr\"]\n",
    "submission = pd.DataFrame({\"SK_ID_CURR\": test_ids, \"TARGET\": test_predictions[:, 1]})\n",
    "\n",
    "submission.to_csv(\"LGBM_dart_v5_submission.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "282e07ca58c09af5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
