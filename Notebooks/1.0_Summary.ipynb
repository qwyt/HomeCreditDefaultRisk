{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Home Credit Default Risk\n",
    "\n",
    "This project is based on Home Credit's [loan application dataset](https://www.kaggle.com/c/home-credit-default-risk/data).\n",
    "\n",
    "Our goal was to advanced boosting to predict which loans Home Credit's portfolio have the highest risk of default or having payment difficulties. To do that:\n",
    "\n",
    "- We've used previous loan applications and applicants history from 3rd parties to build additional features (using Featuretools and manually) in addition to the `applications` dataset.\n",
    "\n",
    "- We were table to achieve an `AUC` of 0.78 using an LGBM tuned with Optuna (we've also built XGBoost and CatBoost models in addition to simple Logit (only on a subset of the dataset) which have achieved comparable or slightly inferior performance) \n",
    "\n",
    "- In addition to estimating the default risk we've built risk-based pricing  and loan quality grading models which allowed to build estimate \"optimal\" interest rates and risk grades for model to optimize overall return on investment.\n",
    "- Our simplified model using fixed `Loss Given Default` ratios, base interest rates and margins (can be adjusted using actual figures from Home Credit) allowed us to build a more conservative loan approval algorithm would allow Home Credit to hypotehticall reduce their loses by up to 80% or so.\n",
    "\n",
    "\n",
    "### Project Structure \n",
    "\n",
    "The project is split into several parts, each represented in separate notebooks. This notebook focuses on the final selection of the \"production\" model and the financial analysis models. Additionally, you might want to look into:\n",
    "\n",
    "- The EDA is available **[here](4.3_EDA.html)** ([with code](4.3_EDA_with_code.html)). It was done parallel with the model design process and show the main reasons behind specific feature selection and engineering decisions.\n",
    "- Feature Analysis is **[here](4.4_All_Features.html)** ([with code](4.4_All_Features_with_code.html)), shows  the statistical distributions and summaries of all individual features used by the final model.\n",
    "\n",
    "Model tuning and selection is available in the appendices:\n",
    "\n",
    "- **[Model Tuning Using Optuna](4.1_Tuning_with_code.html)**: Bayesian tuning was performed for each model, with approximately 100-200 trials using 5-Fold CV.\n",
    "- **[Model Analysis and Selection](4.2_All_Models.html)** ([with code](4.2_All_Models_with_code.html)): We compare multiple tuned LGBM models to select the optimal model for \"production\" and deployment:\n",
    "  - LightGBM GDBT and Dart models using a subset of features selected using SHAP.\n",
    "  - A model using the full features (applications and aggregated applicant past credit history).\n",
    "  - A simplified model only using application data.\n",
    "\n",
    "The selected model was trained using 5-Fold CV on the final dataset, with the data split into:\n",
    "  - 90% `training` sample\n",
    "  - 5% `test` sample\n",
    "  - 5% `probability calibration` sample\n"
   ],
   "id": "63195fc156b9917e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
